{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01990b8a-ce63-496f-9253-f62262e3c60a",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Logistic Regression Model for Absenteeism Prediction</h1>\n",
    "\n",
    "---\n",
    "This notebook develops a **logistic regression model** to predict whether an employee’s absenteeism level is **above the median**—classified as **excessive absenteeism**—based on various personal and workplace-related factors.\n",
    "\n",
    "### Notebook Objectives\n",
    "\n",
    "1. **Load** a dataset that has been cleaned and preprocessed in a prior step.\n",
    "\n",
    "2. **Prepare the data** for modeling:\n",
    "\n",
    "   * Define a binary target variable for excessive absenteeism.\n",
    "   * Select relevant input features and scale them using a custom scaler.\n",
    "\n",
    "3. **Build and fit** a logistic regression model using all available features.\n",
    "\n",
    "4. **Interpret model coefficients** and compute **odds ratios** to identify key predictors.\n",
    "\n",
    "5. **Evaluate model performance** through:\n",
    "\n",
    "   * Summary statistics: AIC, BIC, pseudo R-squared, p-values\n",
    "   * Accuracy scores (train/test split)\n",
    "   * Confusion matrix analysis\n",
    "\n",
    "6. **Test the model** on unseen data and serialize the trained model and scaler for future integration.\n",
    "\n",
    "> A baseline model is first trained using all features. We then apply **backward elimination** to simplify the model by removing statistically insignificant variables, enhancing interpretability while preserving predictive performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d272f51d-6f3a-43dc-b67b-8234852493ac",
   "metadata": {},
   "source": [
    "## **1. Initial Steup and Dataset Load**\n",
    "This section covers the necessary imports and loads the preprocessed dataset, which was prepared in a previous notebook (`1_preprocessing.ipynb`). The data has been cleaned and  encoded appropriately for modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90524bce-811d-421d-b217-6f16d048ce9c",
   "metadata": {},
   "source": [
    "### 1.1 Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db0435d5-af56-4744-af51-6d5b7e32db81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-learn and statsmodels utilities for model training and evaluation\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For saving/loading the model\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Supress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Markdown and display\n",
    "from IPython.display import display, Markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad2c77d-47d9-46bb-9d00-066b912f7405",
   "metadata": {},
   "source": [
    "### 1.2 Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "657910c9-39f8-4c6c-9411-cef00385f599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reason_group_1</th>\n",
       "      <th>reason_group_2</th>\n",
       "      <th>reason_group_3</th>\n",
       "      <th>reason_group_4</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>transportation_expense_dollars</th>\n",
       "      <th>distance_to_work_miles</th>\n",
       "      <th>age</th>\n",
       "      <th>daily_work_load_average</th>\n",
       "      <th>body_mass_index</th>\n",
       "      <th>education</th>\n",
       "      <th>children</th>\n",
       "      <th>pets</th>\n",
       "      <th>absenteeism_time_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>289</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>239.554</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "      <td>50</td>\n",
       "      <td>239.554</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>179</td>\n",
       "      <td>51</td>\n",
       "      <td>38</td>\n",
       "      <td>239.554</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>279</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>239.554</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>289</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>239.554</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reason_group_1  reason_group_2  reason_group_3  reason_group_4  \\\n",
       "0               0               0               0               1   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               1   \n",
       "3               1               0               0               0   \n",
       "4               0               0               0               1   \n",
       "\n",
       "   day_of_week  month  transportation_expense_dollars  distance_to_work_miles  \\\n",
       "0            1      7                             289                      36   \n",
       "1            1      7                             118                      13   \n",
       "2            2      7                             179                      51   \n",
       "3            3      7                             279                       5   \n",
       "4            3      7                             289                      36   \n",
       "\n",
       "   age  daily_work_load_average  body_mass_index  education  children  pets  \\\n",
       "0   33                  239.554               30          0         2     1   \n",
       "1   50                  239.554               31          0         1     0   \n",
       "2   38                  239.554               31          0         0     0   \n",
       "3   39                  239.554               24          0         2     0   \n",
       "4   33                  239.554               30          0         2     1   \n",
       "\n",
       "   absenteeism_time_hours  \n",
       "0                       4  \n",
       "1                       0  \n",
       "2                       2  \n",
       "3                       4  \n",
       "4                       2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessed = pd.read_csv('../data/absenteeism_preprocessed.csv')\n",
    "\n",
    "# Preview the data\n",
    "data_preprocessed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602c4dcf-c93a-4832-a528-715289872086",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb993503-90c4-498d-9c3c-69f6dae05113",
   "metadata": {},
   "source": [
    "## **2. Modeling Preparation**\n",
    "In this section, we prepare the data for modeling by defining the target, selecting features, standardizing inputs, and splitting the data into training and testing sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bc9946-04a7-4e9e-a06e-39c9de05c25b",
   "metadata": {},
   "source": [
    "### 2.1 Define Target Variable\n",
    "\n",
    "Before training our logistic regression model, we need to define a target variable. In our case, we aim to predict whether an employee’s absenteeism is **excessive** based on several features. \n",
    "\n",
    "To convert this into a **binary classification problem**, we define a new target variable called `excessive_absenteeism`, using the following logic:\n",
    "\n",
    "* Take the median of the `absenteeism_time_hours` column.\n",
    "* If an employee’s absenteeism exceeds the median, classify them as `1` (excessively absent).\n",
    "* Otherwise, classify them as `0` (moderately or rarely absent).\n",
    "\n",
    "This approach creates a numerically stable and reasonably balanced dataset. A roughly **50/50 class distribution** ensures that the model doesn’t simply learn to always predict the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00fa5a0f-c453-48bb-b14a-f1714bf1f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_variable(df, target_col='absenteeism_time_hours'):\n",
    "    \n",
    "    \"\"\"Creates a binary target variable based on the median of absenteeism time hours.\n",
    "    Adds a new column 'excessive_absenteeism' to the DataFrame and drops the original column.\"\"\"\n",
    "    \n",
    "    # Calculate median\n",
    "    cutoff = df[target_col].median()\n",
    "    \n",
    "    # Create binary target variable\n",
    "    targets = np.where(df[target_col] > cutoff, 1, 0)\n",
    "    df['excessive_absenteeism'] = targets\n",
    "    \n",
    "    # Check class distribution\n",
    "    proportion_class_1 = targets.sum() / targets.shape[0]\n",
    "    proportion_class_0 = 1 - proportion_class_1\n",
    "    display(Markdown(f\"<div>Class Distribution using cutoff = {cutoff} hours: <strong>{proportion_class_0:.0%}-{proportion_class_1:.0%}</strong></div>\"))\n",
    "    \n",
    "    # Drop the original column\n",
    "    df = df.drop([target_col], axis=1)\n",
    "    \n",
    "    return df, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e149493c-e56c-473b-80d6-e8d0bc5b87b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<div>Class Distribution using cutoff = 3.0 hours: <strong>54%-46%</strong></div>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reason_group_1</th>\n",
       "      <th>reason_group_2</th>\n",
       "      <th>reason_group_3</th>\n",
       "      <th>reason_group_4</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>transportation_expense_dollars</th>\n",
       "      <th>distance_to_work_miles</th>\n",
       "      <th>age</th>\n",
       "      <th>daily_work_load_average</th>\n",
       "      <th>body_mass_index</th>\n",
       "      <th>education</th>\n",
       "      <th>children</th>\n",
       "      <th>pets</th>\n",
       "      <th>excessive_absenteeism</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>289</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>239.554</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "      <td>50</td>\n",
       "      <td>239.554</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>179</td>\n",
       "      <td>51</td>\n",
       "      <td>38</td>\n",
       "      <td>239.554</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>279</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>239.554</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>289</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>239.554</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reason_group_1  reason_group_2  reason_group_3  reason_group_4  \\\n",
       "0               0               0               0               1   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               1   \n",
       "3               1               0               0               0   \n",
       "4               0               0               0               1   \n",
       "\n",
       "   day_of_week  month  transportation_expense_dollars  distance_to_work_miles  \\\n",
       "0            1      7                             289                      36   \n",
       "1            1      7                             118                      13   \n",
       "2            2      7                             179                      51   \n",
       "3            3      7                             279                       5   \n",
       "4            3      7                             289                      36   \n",
       "\n",
       "   age  daily_work_load_average  body_mass_index  education  children  pets  \\\n",
       "0   33                  239.554               30          0         2     1   \n",
       "1   50                  239.554               31          0         1     0   \n",
       "2   38                  239.554               31          0         0     0   \n",
       "3   39                  239.554               24          0         2     0   \n",
       "4   33                  239.554               30          0         2     1   \n",
       "\n",
       "   excessive_absenteeism  \n",
       "0                      1  \n",
       "1                      0  \n",
       "2                      0  \n",
       "3                      1  \n",
       "4                      0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_with_targets, targets = create_target_variable(data_preprocessed)\n",
    "data_with_targets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86adc773-863d-4e2f-b672-c9caddad430c",
   "metadata": {},
   "source": [
    "### 2.2 Select Input Features\n",
    "\n",
    "Before training our logistic regression model, we need to identify the features (**independent variables**) that will be used to predict absenteeism. \n",
    "\n",
    "Since our target variable is `excessive_absenteeism`, we will select all other columns in the DataFrame as input features for the model. This approach includes all predictors initially, while allowing us to remove less informative ones later based on model performance and coefficients. \n",
    "\n",
    "> We hypothesize that:\n",
    ">\n",
    "> * **Reasons for absence** will likely be the strongest predictor of excessive absenteeism.\n",
    "> * **Workload**, **children**, and **pets** may also play a role, as personal responsibilities and work pressure can influence absentee behavior.\n",
    "> * Not all features will have significant predictive power. Logistic regression has the ability to reveal which variables contribute meaningfully to the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3d82d53-8054-4162-ab41-f11fba834512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_input_features(df, columns_to_drop=['excessive_absenteeism']):\n",
    "    \n",
    "    \"\"\"Selects input features for modeling by excluding the target column. \n",
    "    Optionally: drop unnecessary columns through backward elimination.\"\"\"\n",
    "    return df.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5da9c0b-c402-466e-bb8d-2057c9ff7280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reason_group_1</th>\n",
       "      <th>reason_group_2</th>\n",
       "      <th>reason_group_3</th>\n",
       "      <th>reason_group_4</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>transportation_expense_dollars</th>\n",
       "      <th>distance_to_work_miles</th>\n",
       "      <th>age</th>\n",
       "      <th>daily_work_load_average</th>\n",
       "      <th>body_mass_index</th>\n",
       "      <th>education</th>\n",
       "      <th>children</th>\n",
       "      <th>pets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>289</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>239.554</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>118</td>\n",
       "      <td>13</td>\n",
       "      <td>50</td>\n",
       "      <td>239.554</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>179</td>\n",
       "      <td>51</td>\n",
       "      <td>38</td>\n",
       "      <td>239.554</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>279</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>239.554</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>289</td>\n",
       "      <td>36</td>\n",
       "      <td>33</td>\n",
       "      <td>239.554</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reason_group_1  reason_group_2  reason_group_3  reason_group_4  \\\n",
       "0               0               0               0               1   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               1   \n",
       "3               1               0               0               0   \n",
       "4               0               0               0               1   \n",
       "\n",
       "   day_of_week  month  transportation_expense_dollars  distance_to_work_miles  \\\n",
       "0            1      7                             289                      36   \n",
       "1            1      7                             118                      13   \n",
       "2            2      7                             179                      51   \n",
       "3            3      7                             279                       5   \n",
       "4            3      7                             289                      36   \n",
       "\n",
       "   age  daily_work_load_average  body_mass_index  education  children  pets  \n",
       "0   33                  239.554               30          0         2     1  \n",
       "1   50                  239.554               31          0         1     0  \n",
       "2   38                  239.554               31          0         0     0  \n",
       "3   39                  239.554               24          0         2     0  \n",
       "4   33                  239.554               30          0         2     1  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unscaled_inputs = select_input_features(data_with_targets)\n",
    "unscaled_inputs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0ef02-3320-42f4-b9d3-6b4901ef06b3",
   "metadata": {},
   "source": [
    "### 2.3 Standardize the Inputs\n",
    "Standardizing transforms features to have a `mean = 0` and a `standard deviation = 1`, ensuring that all variables contribute equally to the model's learning process and making them more directly comparable.\n",
    "\n",
    ">Important Note:\n",
    "> * We won't standardize the **dummy variables** since they already contain meaningful binary information (`0` = absence of a category, `1` = presence).\n",
    "> * Standardizing them would distort this interpretability, making the model harder to understand.\n",
    "> * To avoid this, we'll use a **custom scaler** that that selectively standardizes numeric features, leaving binary dummies untouched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2262129-2a8f-426a-81ac-22dfe6635447",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns, copy=True, with_mean=True, with_std=True):\n",
    "        # Initialize an internal StandardScaler instance\n",
    "        self.scaler = StandardScaler(copy=copy, with_mean=with_mean, with_std=with_std)\n",
    "        \n",
    "        self.columns = columns  # columns to apply scaling to\n",
    "        self.mean_ = None       # placeholder for column means\n",
    "        self.var_ = None        # placeholder for column variances\n",
    "        self.copy = copy\n",
    "        self.with_mean = with_mean\n",
    "        self.with_std = with_std\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Fit the scaler only on selected columns\n",
    "        self.scaler.fit(X[self.columns], y)\n",
    "        \n",
    "        # Store mean and variance for reference or reuse\n",
    "        self.mean_ = np.mean(X[self.columns])\n",
    "        self.var_ = np.var(X[self.columns])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, copy=None):\n",
    "        init_col_order = X.columns\n",
    "\n",
    "        # Scale selected columns\n",
    "        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n",
    "\n",
    "        # Leave unselected columns unchanged\n",
    "        X_not_scaled = X.loc[:, ~X.columns.isin(self.columns)]\n",
    "\n",
    "        # Combine scaled and unscaled columns, preserving original column order\n",
    "        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad2d2fe4-0186-4399-a5a1-68c9337de9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['reason_group_1' 'reason_group_2' 'reason_group_3' 'reason_group_4'\n",
      " 'day_of_week' 'month' 'transportation_expense_dollars'\n",
      " 'distance_to_work_miles' 'age' 'daily_work_load_average'\n",
      " 'body_mass_index' 'education' 'children' 'pets']\n"
     ]
    }
   ],
   "source": [
    "# Preview available columns\n",
    "print(unscaled_inputs.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c960a24e-830d-4d59-be43-7483bd40cda0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reason_group_1</th>\n",
       "      <th>reason_group_2</th>\n",
       "      <th>reason_group_3</th>\n",
       "      <th>reason_group_4</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>transportation_expense_dollars</th>\n",
       "      <th>distance_to_work_miles</th>\n",
       "      <th>age</th>\n",
       "      <th>daily_work_load_average</th>\n",
       "      <th>body_mass_index</th>\n",
       "      <th>education</th>\n",
       "      <th>children</th>\n",
       "      <th>pets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.683704</td>\n",
       "      <td>0.182726</td>\n",
       "      <td>1.005844</td>\n",
       "      <td>0.412816</td>\n",
       "      <td>-0.536062</td>\n",
       "      <td>-0.806331</td>\n",
       "      <td>0.767431</td>\n",
       "      <td>0</td>\n",
       "      <td>0.880469</td>\n",
       "      <td>0.268487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.683704</td>\n",
       "      <td>0.182726</td>\n",
       "      <td>-1.574681</td>\n",
       "      <td>-1.141882</td>\n",
       "      <td>2.130803</td>\n",
       "      <td>-0.806331</td>\n",
       "      <td>1.002633</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.019280</td>\n",
       "      <td>-0.589690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.007725</td>\n",
       "      <td>0.182726</td>\n",
       "      <td>-0.654143</td>\n",
       "      <td>1.426749</td>\n",
       "      <td>0.248310</td>\n",
       "      <td>-0.806331</td>\n",
       "      <td>1.002633</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.919030</td>\n",
       "      <td>-0.589690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.668253</td>\n",
       "      <td>0.182726</td>\n",
       "      <td>0.854936</td>\n",
       "      <td>-1.682647</td>\n",
       "      <td>0.405184</td>\n",
       "      <td>-0.806331</td>\n",
       "      <td>-0.643782</td>\n",
       "      <td>0</td>\n",
       "      <td>0.880469</td>\n",
       "      <td>-0.589690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.668253</td>\n",
       "      <td>0.182726</td>\n",
       "      <td>1.005844</td>\n",
       "      <td>0.412816</td>\n",
       "      <td>-0.536062</td>\n",
       "      <td>-0.806331</td>\n",
       "      <td>0.767431</td>\n",
       "      <td>0</td>\n",
       "      <td>0.880469</td>\n",
       "      <td>0.268487</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   reason_group_1  reason_group_2  reason_group_3  reason_group_4  \\\n",
       "0               0               0               0               1   \n",
       "1               0               0               0               0   \n",
       "2               0               0               0               1   \n",
       "3               1               0               0               0   \n",
       "4               0               0               0               1   \n",
       "\n",
       "   day_of_week     month  transportation_expense_dollars  \\\n",
       "0    -0.683704  0.182726                        1.005844   \n",
       "1    -0.683704  0.182726                       -1.574681   \n",
       "2    -0.007725  0.182726                       -0.654143   \n",
       "3     0.668253  0.182726                        0.854936   \n",
       "4     0.668253  0.182726                        1.005844   \n",
       "\n",
       "   distance_to_work_miles       age  daily_work_load_average  body_mass_index  \\\n",
       "0                0.412816 -0.536062                -0.806331         0.767431   \n",
       "1               -1.141882  2.130803                -0.806331         1.002633   \n",
       "2                1.426749  0.248310                -0.806331         1.002633   \n",
       "3               -1.682647  0.405184                -0.806331        -0.643782   \n",
       "4                0.412816 -0.536062                -0.806331         0.767431   \n",
       "\n",
       "   education  children      pets  \n",
       "0          0  0.880469  0.268487  \n",
       "1          0 -0.019280 -0.589690  \n",
       "2          0 -0.919030 -0.589690  \n",
       "3          0  0.880469 -0.589690  \n",
       "4          0  0.880469  0.268487  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Avoid scaling dummies and non-numeric categorical features\n",
    "columns_to_omit = ['reason_group_1', 'reason_group_2', 'reason_group_3', 'reason_group_4', 'education']\n",
    "columns_to_scale = [col for col in unscaled_inputs.columns.values if col not in columns_to_omit]\n",
    "\n",
    "# Fit the custom scaler and transform inputs\n",
    "absenteeism_scaler = CustomScaler(columns_to_scale)\n",
    "absenteeism_scaler.fit(unscaled_inputs)\n",
    "scaled_inputs = absenteeism_scaler.transform(unscaled_inputs)\n",
    "\n",
    "# Preview scaled inputs\n",
    "scaled_inputs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca7ecda-626b-48e5-a849-e87d4ef51f4d",
   "metadata": {},
   "source": [
    "### 2.4 Split Data into Train and Test Sets\n",
    "\n",
    "* **Training data (80%)**: Used by the model to learn patterns.\n",
    "* **Test data (20%)**: Used to evaluate the model’s performance on unseen data.\n",
    "\n",
    "> Note on Shuffling:\n",
    ">\n",
    "> * We shuffle the data to remove any order or grouping bias, ensuring randomness in how the training and testing sets are created.\n",
    "> * We also set a `random_state` (seed) of 20 to ensure the split is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c266afb2-5965-4426-84bc-433dabaf6c00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "    Training Set: (560 samples, 14 features) Targets: 560\n",
       "    Testing Set: (140 samples, 14 features) Targets: 140\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the data into training and testing sets (80/20 split)\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    scaled_inputs,\n",
    "    targets,\n",
    "    train_size=0.8,\n",
    "    random_state=20\n",
    ")\n",
    "\n",
    "# Show the shape of the resulting datasets\n",
    "display(Markdown(\n",
    "    f\"\"\"\n",
    "    Training Set: ({x_train.shape[0]} samples, {x_train.shape[1]} features) Targets: {y_train.shape[0]}\n",
    "    Testing Set: ({x_test.shape[0]} samples, {x_test.shape[1]} features) Targets: {y_test.shape[0]}\n",
    "    \"\"\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b9fa33-78c8-407a-9c46-fb6398fcc855",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce3b34e-7a05-47fc-98fc-8fb1e325a9a3",
   "metadata": {},
   "source": [
    "## **3. Logistic Regression Modeling**\n",
    "In this section, we train a logistic regression model using the training data, evaluate its initial performance, and examine statistical summaries to identify potential simplifications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df93a892-1b7c-4ccf-ae7d-92fc6a717a11",
   "metadata": {},
   "source": [
    "### 3.1 Train the Model Using Scikit-Learn\n",
    "We use `LogisticRegression` from scikit-learn to train the model and assess its performance on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35da342d-e615-4928-a6bb-835743dd2662",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(x, y):\n",
    "    \"\"\"Fit logistic regression and calculate the accuracy rate.\"\"\" \n",
    "    reg = LogisticRegression()\n",
    "    reg.fit(x, y)\n",
    "    accuracy = reg.score(x, y)\n",
    "    return reg, accuracy\n",
    "\n",
    "reg, training_accuracy_1 = fit_model(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8080c42a-ae27-4ce8-af41-6dc66980ce47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Training Accuracy:** 77.50%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(f\"**Training Accuracy:** {training_accuracy_1:.2%}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3278f56-024b-4851-bc25-9dd2248ba574",
   "metadata": {},
   "source": [
    "> This means the model **correctly classified 77.5%** of the training observations. This is an acceptable baseline for model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8096983c-e6c0-4587-b96d-fdafdaf997a4",
   "metadata": {},
   "source": [
    "### 3.2 Examine Model Summary with Statsmodels\n",
    "To understand the **statistical significance** of each feature, we fit the same model using `statsmodels`’ `Logit`, which provides detailed coefficient output and p-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "205401f0-cd0e-4d8e-becd-aa5c1cbc8320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.518646\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>  <td>   560</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   546</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>    13</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Mon, 26 May 2025</td> <th>  Pseudo R-squ.:     </th>  <td>0.2467</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>15:41:29</td>     <th>  Log-Likelihood:    </th> <td> -290.44</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -385.55</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.379e-33</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                 <td></td>                   <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>                          <td>   -0.9671</td> <td>    0.136</td> <td>   -7.112</td> <td> 0.000</td> <td>   -1.234</td> <td>   -0.701</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reason_group_1</th>                 <td>    2.2890</td> <td>    0.258</td> <td>    8.880</td> <td> 0.000</td> <td>    1.784</td> <td>    2.794</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reason_group_2</th>                 <td>    1.1549</td> <td>    0.929</td> <td>    1.243</td> <td> 0.214</td> <td>   -0.666</td> <td>    2.976</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>reason_group_3</th>                 <td>    2.9919</td> <td>    0.461</td> <td>    6.489</td> <td> 0.000</td> <td>    2.088</td> <td>    3.896</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>day_of_week</th>                    <td>   -0.0552</td> <td>    0.108</td> <td>   -0.510</td> <td> 0.610</td> <td>   -0.267</td> <td>    0.157</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>month</th>                          <td>    0.1501</td> <td>    0.107</td> <td>    1.401</td> <td> 0.161</td> <td>   -0.060</td> <td>    0.360</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>transportation_expense_dollars</th> <td>    0.6283</td> <td>    0.133</td> <td>    4.741</td> <td> 0.000</td> <td>    0.369</td> <td>    0.888</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>distance_to_work_miles</th>         <td>    0.0093</td> <td>    0.116</td> <td>    0.080</td> <td> 0.936</td> <td>   -0.219</td> <td>    0.237</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>age</th>                            <td>   -0.2197</td> <td>    0.134</td> <td>   -1.636</td> <td> 0.102</td> <td>   -0.483</td> <td>    0.044</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>daily_work_load_average</th>        <td>   -0.0415</td> <td>    0.106</td> <td>   -0.392</td> <td> 0.695</td> <td>   -0.249</td> <td>    0.166</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>body_mass_index</th>                <td>    0.3033</td> <td>    0.132</td> <td>    2.294</td> <td> 0.022</td> <td>    0.044</td> <td>    0.562</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>education</th>                      <td>   -0.3724</td> <td>    0.331</td> <td>   -1.124</td> <td> 0.261</td> <td>   -1.022</td> <td>    0.277</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>children</th>                       <td>    0.3455</td> <td>    0.117</td> <td>    2.948</td> <td> 0.003</td> <td>    0.116</td> <td>    0.575</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>pets</th>                           <td>   -0.3182</td> <td>    0.114</td> <td>   -2.802</td> <td> 0.005</td> <td>   -0.541</td> <td>   -0.096</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/latex": [
       "\\begin{center}\n",
       "\\begin{tabular}{lclc}\n",
       "\\toprule\n",
       "\\textbf{Dep. Variable:}                   &        y         & \\textbf{  No. Observations:  } &      560    \\\\\n",
       "\\textbf{Model:}                           &      Logit       & \\textbf{  Df Residuals:      } &      546    \\\\\n",
       "\\textbf{Method:}                          &       MLE        & \\textbf{  Df Model:          } &       13    \\\\\n",
       "\\textbf{Date:}                            & Mon, 26 May 2025 & \\textbf{  Pseudo R-squ.:     } &   0.2467    \\\\\n",
       "\\textbf{Time:}                            &     15:41:29     & \\textbf{  Log-Likelihood:    } &   -290.44   \\\\\n",
       "\\textbf{converged:}                       &       True       & \\textbf{  LL-Null:           } &   -385.55   \\\\\n",
       "\\textbf{Covariance Type:}                 &    nonrobust     & \\textbf{  LLR p-value:       } & 1.379e-33   \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "\\begin{tabular}{lcccccc}\n",
       "                                          & \\textbf{coef} & \\textbf{std err} & \\textbf{z} & \\textbf{P$> |$z$|$} & \\textbf{[0.025} & \\textbf{0.975]}  \\\\\n",
       "\\midrule\n",
       "\\textbf{const}                            &      -0.9671  &        0.136     &    -7.112  &         0.000        &       -1.234    &       -0.701     \\\\\n",
       "\\textbf{reason\\_group\\_1}                 &       2.2890  &        0.258     &     8.880  &         0.000        &        1.784    &        2.794     \\\\\n",
       "\\textbf{reason\\_group\\_2}                 &       1.1549  &        0.929     &     1.243  &         0.214        &       -0.666    &        2.976     \\\\\n",
       "\\textbf{reason\\_group\\_3}                 &       2.9919  &        0.461     &     6.489  &         0.000        &        2.088    &        3.896     \\\\\n",
       "\\textbf{day\\_of\\_week}                    &      -0.0552  &        0.108     &    -0.510  &         0.610        &       -0.267    &        0.157     \\\\\n",
       "\\textbf{month}                            &       0.1501  &        0.107     &     1.401  &         0.161        &       -0.060    &        0.360     \\\\\n",
       "\\textbf{transportation\\_expense\\_dollars} &       0.6283  &        0.133     &     4.741  &         0.000        &        0.369    &        0.888     \\\\\n",
       "\\textbf{distance\\_to\\_work\\_miles}        &       0.0093  &        0.116     &     0.080  &         0.936        &       -0.219    &        0.237     \\\\\n",
       "\\textbf{age}                              &      -0.2197  &        0.134     &    -1.636  &         0.102        &       -0.483    &        0.044     \\\\\n",
       "\\textbf{daily\\_work\\_load\\_average}       &      -0.0415  &        0.106     &    -0.392  &         0.695        &       -0.249    &        0.166     \\\\\n",
       "\\textbf{body\\_mass\\_index}                &       0.3033  &        0.132     &     2.294  &         0.022        &        0.044    &        0.562     \\\\\n",
       "\\textbf{education}                        &      -0.3724  &        0.331     &    -1.124  &         0.261        &       -1.022    &        0.277     \\\\\n",
       "\\textbf{children}                         &       0.3455  &        0.117     &     2.948  &         0.003        &        0.116    &        0.575     \\\\\n",
       "\\textbf{pets}                             &      -0.3182  &        0.114     &    -2.802  &         0.005        &       -0.541    &       -0.096     \\\\\n",
       "\\bottomrule\n",
       "\\end{tabular}\n",
       "%\\caption{Logit Regression Results}\n",
       "\\end{center}"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                  560\n",
       "Model:                          Logit   Df Residuals:                      546\n",
       "Method:                           MLE   Df Model:                           13\n",
       "Date:                Mon, 26 May 2025   Pseudo R-squ.:                  0.2467\n",
       "Time:                        15:41:29   Log-Likelihood:                -290.44\n",
       "converged:                       True   LL-Null:                       -385.55\n",
       "Covariance Type:            nonrobust   LLR p-value:                 1.379e-33\n",
       "==================================================================================================\n",
       "                                     coef    std err          z      P>|z|      [0.025      0.975]\n",
       "--------------------------------------------------------------------------------------------------\n",
       "const                             -0.9671      0.136     -7.112      0.000      -1.234      -0.701\n",
       "reason_group_1                     2.2890      0.258      8.880      0.000       1.784       2.794\n",
       "reason_group_2                     1.1549      0.929      1.243      0.214      -0.666       2.976\n",
       "reason_group_3                     2.9919      0.461      6.489      0.000       2.088       3.896\n",
       "day_of_week                       -0.0552      0.108     -0.510      0.610      -0.267       0.157\n",
       "month                              0.1501      0.107      1.401      0.161      -0.060       0.360\n",
       "transportation_expense_dollars     0.6283      0.133      4.741      0.000       0.369       0.888\n",
       "distance_to_work_miles             0.0093      0.116      0.080      0.936      -0.219       0.237\n",
       "age                               -0.2197      0.134     -1.636      0.102      -0.483       0.044\n",
       "daily_work_load_average           -0.0415      0.106     -0.392      0.695      -0.249       0.166\n",
       "body_mass_index                    0.3033      0.132      2.294      0.022       0.044       0.562\n",
       "education                         -0.3724      0.331     -1.124      0.261      -1.022       0.277\n",
       "children                           0.3455      0.117      2.948      0.003       0.116       0.575\n",
       "pets                              -0.3182      0.114     -2.802      0.005      -0.541      -0.096\n",
       "==================================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stats_summary_table(x_train, y_train, columns_to_drop=['reason_group_4']):\n",
    "    \"\"\"Generate statsmodels logistic regression summary for feature evaluation.\"\"\"\n",
    "    x_train_sm = sm.add_constant(x_train.copy())\n",
    "    x_train_sm = x_train_sm.drop(columns=columns_to_drop)  # Drop one dummy to prevent multicollinearity\n",
    "    logit_model = sm.Logit(y_train, x_train_sm)\n",
    "    result = logit_model.fit()\n",
    "    return result\n",
    "\n",
    "model_1 = stats_summary_table(x_train, y_train)\n",
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9d4461-ab3c-4388-8369-8b00b36051a0",
   "metadata": {},
   "source": [
    "#### Interpretation of the Model Summary:\n",
    "\n",
    "* The **Pseudo R²** is approximately 0.2467, meaning the model explains **~24.7%** of the variation in the log-odds.\n",
    "* In logistic regression, values between **0.2-0.4** for Pseudo R² indicate a **reasonable model fit**.\n",
    "* Some features are not statistically significant at **p-values > 0.05** level.\n",
    "* Removing such features may improve model simplicity and generalizability without major performance loss.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057cd02e-a5d2-4738-82a8-74b68b67dedf",
   "metadata": {},
   "source": [
    "### 3.3 Interpret Coefficients and Odds Ratios\n",
    "To better understand the logistic regression model, we examine the coefficients and what they tell us about each feature’s impact on the odds of excessive absenteeism.\n",
    "\n",
    "In logistic regression, the coefficients represent **log-odds**, which are not easy to interpret directly. By exponentiating these coefficients, we convert them to **odds ratios**, making interpretation more intuitive:\n",
    "\n",
    "> ##### Key Interpretation Guidelines:\n",
    ">\n",
    "> * **Odds Ratio ≈ 1 or Coefficient ≈ 0** → Little to no effect on outcome\n",
    "> * **Odds Ratio > 1** → Increases the odds of excessive absenteeism\n",
    "> * **Odds Ratio < 1** → Decreases the odds of excessive absenteeism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e9281b2-d892-4baa-af6b-a7127c295b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature Name</th>\n",
       "      <th>Coefficient</th>\n",
       "      <th>Odds Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>reason_group_3</td>\n",
       "      <td>3.096739</td>\n",
       "      <td>22.125672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reason_group_1</td>\n",
       "      <td>2.801363</td>\n",
       "      <td>16.467081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reason_group_2</td>\n",
       "      <td>0.933541</td>\n",
       "      <td>2.543499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>reason_group_4</td>\n",
       "      <td>0.857183</td>\n",
       "      <td>2.356513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>transportation_expense_dollars</td>\n",
       "      <td>0.613216</td>\n",
       "      <td>1.846359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>children</td>\n",
       "      <td>0.361898</td>\n",
       "      <td>1.436052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>body_mass_index</td>\n",
       "      <td>0.271155</td>\n",
       "      <td>1.311478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>month</td>\n",
       "      <td>0.166403</td>\n",
       "      <td>1.181049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>daily_work_load_average</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>0.999923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>distance_to_work_miles</td>\n",
       "      <td>-0.007779</td>\n",
       "      <td>0.992251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>day_of_week</td>\n",
       "      <td>-0.084316</td>\n",
       "      <td>0.919141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>age</td>\n",
       "      <td>-0.165545</td>\n",
       "      <td>0.847431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>education</td>\n",
       "      <td>-0.206027</td>\n",
       "      <td>0.813811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>pets</td>\n",
       "      <td>-0.285729</td>\n",
       "      <td>0.751466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Intercept</td>\n",
       "      <td>-1.656628</td>\n",
       "      <td>0.190781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Feature Name  Coefficient  Odds Ratio\n",
       "3                   reason_group_3     3.096739   22.125672\n",
       "1                   reason_group_1     2.801363   16.467081\n",
       "2                   reason_group_2     0.933541    2.543499\n",
       "4                   reason_group_4     0.857183    2.356513\n",
       "7   transportation_expense_dollars     0.613216    1.846359\n",
       "13                        children     0.361898    1.436052\n",
       "11                 body_mass_index     0.271155    1.311478\n",
       "6                            month     0.166403    1.181049\n",
       "10         daily_work_load_average    -0.000077    0.999923\n",
       "8           distance_to_work_miles    -0.007779    0.992251\n",
       "5                      day_of_week    -0.084316    0.919141\n",
       "9                              age    -0.165545    0.847431\n",
       "12                       education    -0.206027    0.813811\n",
       "14                            pets    -0.285729    0.751466\n",
       "0                        Intercept    -1.656628    0.190781"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def summarize_logistic_regression(model):\n",
    "    \"\"\"\n",
    "    Create a summary table showing the intercept and coefficients\n",
    "    of a trained logistic regression model.\n",
    "    \"\"\"\n",
    "    # Extract feature names\n",
    "    feature_names = unscaled_inputs.columns.values\n",
    "    summary_table = pd.DataFrame({'Feature Name': feature_names})\n",
    "    \n",
    "    # Add coefficients and odds ratios\n",
    "    summary_table['Coefficient'] = np.transpose(model.coef_)\n",
    "    summary_table.index += 1\n",
    "    summary_table.loc[0] = ['Intercept', model.intercept_[0]]\n",
    "\n",
    "    # Calculate odds ratios\n",
    "    summary_table['Odds Ratio'] = np.exp(summary_table['Coefficient'])\n",
    "\n",
    "    # Sort by odds ratio\n",
    "    display(summary_table.sort_values('Odds Ratio', ascending=False))\n",
    "\n",
    "summarize_logistic_regression(reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84c88ce-02bd-4fb0-9258-46605f1c2acf",
   "metadata": {},
   "source": [
    "---\n",
    "<h3 align='center'>Most Influential Features</h3>\n",
    "\n",
    "| Feature                        | Odds Ratio | Interpretation                                                                                             |\n",
    "| :----------------------------- | ---------: | :--------------------------------------------------------------------------------------------------------- |\n",
    "| **reason\\_group\\_3**           |      22.13 | **Strongest predictor**. Individuals citing this reason are **22x more likely** to be excessively absent. |\n",
    "| **reason\\_group\\_1**           |      16.47 | Major predictor. **16x higher odds** of excessive absence when this reason applies.                       |\n",
    "| **reason\\_group\\_2**           |       2.54 | Moderate effect. **2.5x more likely** to be excessively absent.                                           |\n",
    "| **reason\\_group\\_4**           |       2.36 | Slightly increases odds. Over **2x** the baseline odds.                                                   |\n",
    "| **transportation\\_expense**    |       1.85 | Each standard deviation increase in expense increases absenteeism odds by **\\~85%**.                       |\n",
    "| **children**                   |       1.44 | Employees with more children are **43% more likely** to be excessively absent.                             |\n",
    "| **body\\_mass\\_index**          |       1.31 | Higher BMI slightly increases absenteeism odds **\\~31% higher odds**.                                    |\n",
    "| **pets**                       |       0.75 | **Significant**. Each standard deviation increase in pet ownership **reduces odds by \\~25%**.             |\n",
    "| **age**                        |       0.85 | Older employees are **\\~15% less likely** to be excessively absent.                                        |\n",
    "\n",
    "#### Summary:\n",
    "* Strongest predictors of absenteeism include specific absence reasons (`reason_group_3`, `reason_group_1`), transportation expense, and children.\n",
    "* Pets and age appear to decrease the odds of absenteeism.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217edd6a-4de8-4028-bb33-13774c70d47f",
   "metadata": {},
   "source": [
    "### 3.4 Simplify Model with Backward Elimination\n",
    "\n",
    "Although some of these features may hold predictive value in different contexts or datasets, we aim for a leaner model in this iteration. To improve interpretability and generalizability, we simplify the model by removing features with:\n",
    "\n",
    "* **High p-values** (i.e., low statistical significance)\n",
    "* **Odds Ratios close to 1** (i.e., minimal impact on the outcome)\n",
    "\n",
    "<h4 align=\"center\">Feature Consideration for Elimination</h4>\n",
    "\n",
    "| Feature                   | P-Value | Odds Ratio | Rationale                                               |\n",
    "| ------------------------- | ------- | ---------- | ------------------------------------------------------- |\n",
    "| `distance_to_work_miles`  | 0.9362  | 0.992251   | **Highest p-value and weak influence**                  |\n",
    "| `daily_work_load_average` | 0.6953  | 0.999923   | Statistically insignificant and weak influence.         |\n",
    "| `day_of_week`             | 0.6102  | 0.919141   | Statistically insignificant and weak influence.         |\n",
    "| `education`               | 0.2611  | 0.813811   | Slight  effect, but statistically insignificant.        |\n",
    "| `month`                   | 0.1613  | 1.181049   | May retain for potential seasonal effects.              |\n",
    "| `reason_group_2`          | 0.2139  | 2.543499   | Moderate influence. Retained for categorical balance.   |\n",
    "| `age`                     | 0.1018  | 0.847431   | Moderate influence. Retained temporarily.               |\n",
    "\n",
    "#### **Model Refinement (Feature Dropping):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bf1952e6-b516-424c-b30b-e460a6017f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.520303\n",
      "         Iterations 6\n"
     ]
    }
   ],
   "source": [
    "# Input selection\n",
    "columns_to_drop = ['excessive_absenteeism', 'day_of_week', 'daily_work_load_average', 'distance_to_work_miles', 'education']\n",
    "unscaled_inputs = select_input_features(data_with_targets, columns_to_drop)\n",
    "\n",
    "# Scale the input\n",
    "columns_to_omit = ['reason_group_1', 'reason_group_2', 'reason_group_3', 'reason_group_4', 'education']\n",
    "columns_to_scale = [col for col in unscaled_inputs.columns.values if col not in columns_to_omit]\n",
    "absenteeism_scaler = CustomScaler(columns_to_scale)\n",
    "absenteeism_scaler.fit(unscaled_inputs)\n",
    "scaled_inputs = absenteeism_scaler.transform(unscaled_inputs)\n",
    "\n",
    "# Split into train and test dataset\n",
    "x_train, x_test, y_train, y_test = train_test_split(scaled_inputs, targets, train_size=0.8, random_state=20)\n",
    "\n",
    "# Fit the model and generate metrics\n",
    "reg, training_accuracy_2 = fit_model(x_train, y_train)\n",
    "model_2 = stats_summary_table(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1669b1f7-9dbf-4c92-b5eb-8a8c3cf39243",
   "metadata": {},
   "source": [
    "\n",
    "<h4 align=\"center\">Summary of Backward Elimination</h4>\n",
    "\n",
    "| Iteration | Accuracy Δ | Pseudo R² Δ | AIC Δ ↓ |  BIC Δ ↓ | Notes                                                  |\n",
    "| --------: | ---------: | ----------: | ------: | -------: | ------------------------------------------------------ |\n",
    "|         1 | -0.0018 |  -0.0006 | -5.56 | -18.54 | Minor simplification; negligible impact                          |\n",
    "|         2 |  +0.0036 |  -0.0024 | -6.14 | -23.45 | **Best trade-off** — simpler, slightly better accuracy          |\n",
    "|         3 |   0.0000 |  -0.0054 | -5.81 | -27.45 | No gain in accuracy; more drop in R²                            |\n",
    "|         4 | -0.0036 |  -0.0040 | -6.92 | -28.56 | Performance degrades                                             |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75de2538-3a08-48e8-81f4-85d8291f3bcd",
   "metadata": {},
   "source": [
    "#### **Best Iteration — Iteration 2:**\n",
    "In this version, we dropped: `distance_to_work_miles`, `daily_work_load_average`, `day_of_week`, and `education`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7699f39a-5725-4d94-94c5-43924afe1089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric         Model 1        Model 2\n",
      "-------------------------------------\n",
      "Accuracy       0.7750         0.7786\n",
      "Pseudo R²      0.2467         0.2443\n",
      "AIC            608.88         602.74\n",
      "BIC            669.47         646.02\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Metric':<15}{'Model 1':<15}{'Model 2'}\")\n",
    "print(\"-------------------------------------\")\n",
    "print(f\"{'Accuracy':<15}{training_accuracy_1:<15.4f}{training_accuracy_2:.4f}\")\n",
    "print(f\"{'Pseudo R²':<15}{model_1.prsquared:<15.4f}{model_2.prsquared:.4f}\")\n",
    "print(f\"{'AIC':<15}{model_1.aic:<15.2f}{model_2.aic:.2f}\")\n",
    "print(f\"{'BIC':<15}{model_1.bic:<15.2f}{model_2.bic:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4a1d43-b1cd-4785-b354-5769836f7402",
   "metadata": {},
   "source": [
    "#### Results:\n",
    "\n",
    "* **Accuracy** improves to 77.86% — the best among all iterations.\n",
    "* **AIC/BIC** both decrease significantly (by -6.14 and -23.45 respectively), indicating a **simpler** and likely more **generalizable** model.\n",
    "* **Pseudo R²** drops only slightly (**0.2443 vs. 0.2467**), suggesting minimal loss in explanatory power.\n",
    "\n",
    "> *This trade-off favors simplicity without sacrificing performance.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10897b7-c038-46ad-af51-e428d3e2dcb3",
   "metadata": {},
   "source": [
    "## **4. Testing the Model**\n",
    "In this final stage, we evaluate our model's performance on **unseen data** that the model has never encountered during training. This allows us to measure the model’s ability to generalize to real-world scenarios rather than just memorizing the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5952862c-a0cb-4af2-b629-d447953842c8",
   "metadata": {},
   "source": [
    "### 4.1 Model Accuracy (Test Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "838b3cbb-49f5-4682-867d-e7980622ab4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Test Accuracy:** 75.00%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_accuracy = reg.score(x_test, y_test)\n",
    "display(Markdown(f\"**Test Accuracy:** {test_accuracy:.2%}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3300be79-f57a-499f-963a-056499a41327",
   "metadata": {},
   "source": [
    "> Based on the test dataset, the model correctly predicts **~75%** of the time.\n",
    "\n",
    "- The **training accuracy** was approximately **77.86%**, so this small **drop is expected** and a sign of good generalization.\n",
    "- A drop less than 10–20% suggests that the model is **not overfitting** or specializing to the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9bdb12-aa08-412f-a27b-500a78939b83",
   "metadata": {},
   "source": [
    "### 4.2 Confusion Matrix\n",
    "Let’s now evaluate the model's classification performance using a confusion matrix, which shows how many predictions were correct vs incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b7b5ea69-248f-4ba0-845d-5f4d76685f0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted 0</th>\n",
       "      <th>Predicted 1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual 0</th>\n",
       "      <td>59.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual 1</th>\n",
       "      <td>20.0</td>\n",
       "      <td>46.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Predicted 0  Predicted 1\n",
       "Actual 0         59.0         15.0\n",
       "Actual 1         20.0         46.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actual class labels\n",
    "actual_values = y_test\n",
    "\n",
    "# Predicted class labels\n",
    "pred_values = reg.predict(x_test)\n",
    "\n",
    "# Binning for confusion matrix (0: class 0, 1: class 1)\n",
    "bins = np.array([0, 0.5, 1])\n",
    "\n",
    "# Create confusion matrix using 2D histogram\n",
    "cm = np.histogram2d(actual_values, pred_values, bins=bins)[0]\n",
    "\n",
    "# Convert to DataFrame for better presentation\n",
    "confusion_matrix = pd.DataFrame(cm, index=['Actual 0', 'Actual 1'], columns=['Predicted 0', 'Predicted 1'])\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c664f477-42cf-4547-9d88-2a3a1b21199f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Misclassification Rate:** 25.00%"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Misclassification rate\n",
    "misclassification_rate = (cm[0][1] + cm[1][0]) / cm.sum()\n",
    "display(Markdown(f\"**Misclassification Rate:** {misclassification_rate:.2%}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5b9006-9395-4792-9a3a-073ea64d97e7",
   "metadata": {},
   "source": [
    "This means that about 1 in 4 predictions are incorrect. It is still reasonable given the context and the limited number of highly influential predictors. Further tuning, feature engineering, or additional data could reduce this in the future.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d19eb94-bf2a-48b1-a635-cb6cf7c23e64",
   "metadata": {},
   "source": [
    "## **5. Saving the Model and Scaler**\n",
    "Once a machine learning model has been trained, saving the model is a critical step in a machine learning pipeline. Instead of re-training the model every time we need to make predictions, we can **serialize** the trained model into a compact file that can be loaded and used instantly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7313fae-0e39-4843-a528-c68d206821ea",
   "metadata": {},
   "source": [
    "### 5.1 What is Being Saved\n",
    "\n",
    "The object `reg` (our trained logistic regression model) holds all necessary model attributes:\n",
    "\n",
    "* Coefficients and intercept\n",
    "* Random state\n",
    "* Everything needed to make future predictions\n",
    "\n",
    "Likewise, the object `absenteeism_scaler` stores:\n",
    "\n",
    "* The features that were scaled\n",
    "* The mean and standard deviation of each feature\n",
    "* The standardization logic required to process new input data identically to the training data\n",
    "\n",
    "To ensure predictions on future data are consistent, both the model and the scaler must be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef93504-bd66-4459-ad1d-a24ef5592251",
   "metadata": {},
   "source": [
    "### 5.2 Pickling\n",
    "\n",
    "We use Python’s built-in `pickle` module to serialize (`pickle`) and later restore (`unpickle`) these objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "55c50be2-224d-42be-9650-389831b46dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder if it doesn't exist\n",
    "os.makedirs('../integration/model_artifacts', exist_ok=True)\n",
    "\n",
    "# Save the trained model\n",
    "with open('../integration/model_artifacts/model', 'wb') as file:\n",
    "    pickle.dump(reg, file)\n",
    "\n",
    "# Save the scaler\n",
    "with open('../integration/model_artifacts/scaler', 'wb') as file:\n",
    "    pickle.dump(absenteeism_scaler, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6bca2e-a2d3-429d-be1f-2b4ef3b85ed1",
   "metadata": {},
   "source": [
    "Now, these files can now be reloaded anytime using `pickle.load()` and used for predictions without re-training the model or re-defining the scaler logic.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6d05aa-ded9-425f-bfa4-ff7f08b0c5ec",
   "metadata": {},
   "source": [
    "## **6. Summary**\n",
    "\n",
    "We successfully built and validated a logistic regression model to predict excessive absenteeism. Below is a structured overview of the modeling pipeline:\n",
    "\n",
    "---\n",
    "\n",
    "### **6.1 Steps Completed**\n",
    "\n",
    "#### **6.1.1 Model Preparation**\n",
    "\n",
    "* Loaded the preprocessed absenteeism dataset.\n",
    "* Created a binary target variable, `excessive_absenteeism`, with an approximately **50/50 class distribution**.\n",
    "* Selected relevant input features for modeling.\n",
    "* Applied a **custom scaler** (built on `StandardScaler`) to scale only numerical variables.\n",
    "* Split the data into **training and test sets (80/20 split)**.\n",
    "\n",
    "#### **6.1.2 Model Training**\n",
    "\n",
    "* Trained an initial logistic regression model using the training data.\n",
    "* Interpreted model outputs using both `statsmodels` and `scikit-learn`.\n",
    "* Identified key predictors based on **odds ratios**, including:\n",
    "\n",
    "  * `reason_group_3` (\\~22x more likely)\n",
    "  * `reason_group_1` (\\~16x more likely)\n",
    "  * Other influential features: `transportation_expense_dollars` (\\~85% more likely), `children` (\\~43% more likely), etc.\n",
    "\n",
    "#### **6.1.3 Feature Selection: Backward Elimination**\n",
    "\n",
    "* Evaluated feature significance using **p-values** and **odds ratios**.\n",
    "* Removed statistically insignificant features iteratively.\n",
    "* Simplified the model for better interpretability without sacrificing accuracy.\n",
    "\n",
    "#### **6.1.4 Model Evaluation**\n",
    "\n",
    "* **Training Accuracy:** \\~77.86%\n",
    "* **Test Accuracy:** \\~75.00%\n",
    "* Minimal performance gap suggests **no overfitting** and strong generalization.\n",
    "* Evaluated performance using a **confusion matrix**.\n",
    "\n",
    "#### **6.1.5 Model & Scaler Serialization**\n",
    "\n",
    "* Serialized the trained model (`reg`) using `pickle`.\n",
    "* Saved the **custom scaler** (`absenteeism_scaler`) to ensure consistent preprocessing for future predictions.\n",
    "\n",
    "---\n",
    "\n",
    "### **6.2 Next Step: Integration**\n",
    "\n",
    "In the next phase, we will apply this model to new data by:\n",
    "\n",
    "* **Loading the saved model and scaler**.\n",
    "* **Generating predictions** on incoming employee absenteeism records.\n",
    "* **Preparing output** for downstream analysis in BI tools like **Tableau**.\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
